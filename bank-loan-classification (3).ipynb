{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"In financial services, accurate prediction of loan repayment outcomes holds paramount importance. This dataset contains a collection of key variables associated with loan applicants. These variables provide valuable insights into the applicant's financial profile, employment history, credit behavior, and loan-specific details.\n\nThe goal is to train the best machine learning model to maximize the predictive capability of deeply understanding the past customer’s profile minimizing the risk of future loan defaults. By analyzing the relationships between borrower attributes and loan repayment outcomes, the model will enable lending institutions to make well-informed decisions that balance profitability and risk management.","metadata":{}},{"cell_type":"markdown","source":"### Understanding the Variables","metadata":{}},{"cell_type":"markdown","source":"1. id: Unique ID of the loan application.\n\n2. grade: LC assigned loan grade.\n\n3. annual_inc: The self-reported annual income provided by the borrower during registration.\n\n4. short_emp: 1 when employed for 1 year or less.\n\n5. emp_length_num: Employment length in years. Possible values are - between 0 and 10 where 0 means less than one year and      10 means ten or more years.\n\n6. home_ownership: Type of home ownership.\n\n7. dti (Debt-To-Income Ratio): A ratio calculated using the borrower’s total monthly debt payments on the total debt            obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.\n\n8. purpose: A category provided by the borrower for the loan request.\n\n9. term: The number of payments on the loan. Values are in months and can be either 36 or 60.\n\n10. last_delinq_none: 1 when the borrower had at least one event of delinquency.\n\n11. last_major_derog_none: 1 borrower had at least 90 days of a bad rating.\n\n12. revol_util: Revolving line utilization rate, or the amount of credit the borrower is using relative to all available         revolving credit.\n\n13. total_rec_late_fee: Late fees received to date.\n\n14. od_ratio: Overdraft ratio.\n\n15. bad_loan: 1 when a loan was not paid.","metadata":{}},{"cell_type":"code","source":"# Import libraries. begin, let's import the necessary libraries that we'll be using throughout this notebook:\n\n# Data Manipulation Libraries\nimport numpy as np \nimport pandas as pd \n\n# Data Visualization Libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Machine Learning Libraries\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, precision_score, confusion_matrix, precision_recall_curve, average_precision_score \nfrom sklearn.model_selection import GridSearchCV\n\n# Data Resampling Libraries\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import TomekLinks\n\n# Machine Learning Models\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-16T11:39:25.516302Z","iopub.execute_input":"2023-08-16T11:39:25.516714Z","iopub.status.idle":"2023-08-16T11:39:25.525026Z","shell.execute_reply.started":"2023-08-16T11:39:25.516682Z","shell.execute_reply":"2023-08-16T11:39:25.523900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# knowing the name of the dataset.\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:25.529488Z","iopub.execute_input":"2023-08-16T11:39:25.529871Z","iopub.status.idle":"2023-08-16T11:39:25.551864Z","shell.execute_reply.started":"2023-08-16T11:39:25.529841Z","shell.execute_reply":"2023-08-16T11:39:25.551094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load tha data.\ndf = pd.read_csv(\"/kaggle/input/machine-learning/lending_club_loan_dataset.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:25.553539Z","iopub.execute_input":"2023-08-16T11:39:25.554064Z","iopub.status.idle":"2023-08-16T11:39:25.619558Z","shell.execute_reply.started":"2023-08-16T11:39:25.554034Z","shell.execute_reply":"2023-08-16T11:39:25.618767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparation the data","metadata":{}},{"cell_type":"code","source":"# Seeing the shape of the data.\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:25.620874Z","iopub.execute_input":"2023-08-16T11:39:25.621395Z","iopub.status.idle":"2023-08-16T11:39:25.627341Z","shell.execute_reply.started":"2023-08-16T11:39:25.621366Z","shell.execute_reply":"2023-08-16T11:39:25.626320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seeing if there are dublicated.\ndf.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:25.630293Z","iopub.execute_input":"2023-08-16T11:39:25.630610Z","iopub.status.idle":"2023-08-16T11:39:25.660344Z","shell.execute_reply.started":"2023-08-16T11:39:25.630583Z","shell.execute_reply":"2023-08-16T11:39:25.659251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# seeing if there are null values.\ndf.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:25.661722Z","iopub.execute_input":"2023-08-16T11:39:25.662077Z","iopub.status.idle":"2023-08-16T11:39:25.697665Z","shell.execute_reply.started":"2023-08-16T11:39:25.662049Z","shell.execute_reply":"2023-08-16T11:39:25.696447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In our dataset, we've identified missing data in three columns: \"homeownership,\" \"dti,\" and \"last_major_derog_none.\" With 1491 null values in \"homeownership\" and 154 in \"dti,\" and a substantial 97% (19426) of missing values in \"last_major_derog_none,\" we have options for managing these gaps without significantly compromising accuracy.\n\nGiven our dataset's size of 20,000 rows, we can reasonably remove the rows with missing values in \"homeownership\" and \"dti.\" This removal should have a minimal impact on the overall accuracy and integrity of our data.\n\nHowever, due to the overwhelming 97% missing values in \"last_major_derog_none,\" it appears reasonable to drop this column entirely from our analysis. Relying on it could introduce noise and bias into our analysis, impacting the reliability of our results.","metadata":{}},{"cell_type":"code","source":"# Drop last_major_derog_none and id.\ndf.drop(['last_major_derog_none', 'id'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:25.699502Z","iopub.execute_input":"2023-08-16T11:39:25.700022Z","iopub.status.idle":"2023-08-16T11:39:25.707610Z","shell.execute_reply.started":"2023-08-16T11:39:25.699977Z","shell.execute_reply":"2023-08-16T11:39:25.706796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop the rows that have null values in home_ownership.\ndf.dropna(subset = [\"home_ownership\", \"dti\"], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:25.709068Z","iopub.execute_input":"2023-08-16T11:39:25.709406Z","iopub.status.idle":"2023-08-16T11:39:25.731898Z","shell.execute_reply.started":"2023-08-16T11:39:25.709378Z","shell.execute_reply":"2023-08-16T11:39:25.730691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:25.733285Z","iopub.execute_input":"2023-08-16T11:39:25.734228Z","iopub.status.idle":"2023-08-16T11:39:25.767061Z","shell.execute_reply.started":"2023-08-16T11:39:25.734198Z","shell.execute_reply":"2023-08-16T11:39:25.766140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seeing information about data.\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:25.770520Z","iopub.execute_input":"2023-08-16T11:39:25.771101Z","iopub.status.idle":"2023-08-16T11:39:25.808317Z","shell.execute_reply.started":"2023-08-16T11:39:25.771071Z","shell.execute_reply":"2023-08-16T11:39:25.807045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seeing the unique values in the grade.\ndf['grade'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:25.809885Z","iopub.execute_input":"2023-08-16T11:39:25.810288Z","iopub.status.idle":"2023-08-16T11:39:25.820850Z","shell.execute_reply.started":"2023-08-16T11:39:25.810256Z","shell.execute_reply":"2023-08-16T11:39:25.819769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We currently have data divided into 7 distinct groups in the \"grade\". To achieve a more balanced and optimal distribution, we will condense these groups into 5 categories. This adjustment aims to create a more refined and manageable data segmentation for analysis and decision-making.  ","metadata":{}},{"cell_type":"code","source":"# Define a mapping dictionary to combine the clusters\ncluster_mapping = {\"E\":\"E\", \"F\":\"E\" ,\"G\":\"E\"}\n\n# Update the \"grade\" column with the new cluster labels\ndf['grade'] = df['grade'].replace(cluster_mapping)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:25.822452Z","iopub.execute_input":"2023-08-16T11:39:25.822891Z","iopub.status.idle":"2023-08-16T11:39:25.838603Z","shell.execute_reply.started":"2023-08-16T11:39:25.822856Z","shell.execute_reply":"2023-08-16T11:39:25.837521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['grade'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:25.840129Z","iopub.execute_input":"2023-08-16T11:39:25.840487Z","iopub.status.idle":"2023-08-16T11:39:25.852141Z","shell.execute_reply.started":"2023-08-16T11:39:25.840458Z","shell.execute_reply":"2023-08-16T11:39:25.851057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seeing the unique values in the home_ownership.\ndf['home_ownership'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:25.853809Z","iopub.execute_input":"2023-08-16T11:39:25.854133Z","iopub.status.idle":"2023-08-16T11:39:25.868592Z","shell.execute_reply.started":"2023-08-16T11:39:25.854106Z","shell.execute_reply":"2023-08-16T11:39:25.867809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seeing the unique values in the purpose.\ndf['purpose'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:25.869965Z","iopub.execute_input":"2023-08-16T11:39:25.870264Z","iopub.status.idle":"2023-08-16T11:39:25.885527Z","shell.execute_reply.started":"2023-08-16T11:39:25.870238Z","shell.execute_reply":"2023-08-16T11:39:25.884446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We currently have data divided into 12 distinct groups in the \"purpose\".  To enhance clarity and streamline analysis, we have consolidated these groups into 6 broader categories by merging those with similar characteristics. This consolidation maintains the essence of the original classification while simplifying the representation for more effective interpretation.","metadata":{}},{"cell_type":"code","source":"# Define a mapping dictionary to combine the clusters\ncluster_mapping = {\n    \"wedding\": \"entertainment\",\n    \"vacation\": \"entertainment\",\n    \"moving\": \"entertainment\",\n    \"car\": \"entertainment\",\n    \"major_purchase\": \"projects\",\n    \"small_business\": \"projects\",\n    \"house\": \"projects\",\n    \"medical\": \"projects\"\n}\n\n# Update the \"purpose\" column with the new cluster labels using map\ndf['purpose'] = df['purpose'].replace(cluster_mapping)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:25.887275Z","iopub.execute_input":"2023-08-16T11:39:25.887699Z","iopub.status.idle":"2023-08-16T11:39:25.910314Z","shell.execute_reply.started":"2023-08-16T11:39:25.887661Z","shell.execute_reply":"2023-08-16T11:39:25.909160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['purpose'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:25.912040Z","iopub.execute_input":"2023-08-16T11:39:25.912793Z","iopub.status.idle":"2023-08-16T11:39:25.923590Z","shell.execute_reply.started":"2023-08-16T11:39:25.912728Z","shell.execute_reply":"2023-08-16T11:39:25.922564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['term'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:25.925068Z","iopub.execute_input":"2023-08-16T11:39:25.925601Z","iopub.status.idle":"2023-08-16T11:39:25.941273Z","shell.execute_reply.started":"2023-08-16T11:39:25.925572Z","shell.execute_reply":"2023-08-16T11:39:25.940098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Among the variables, there appears to be a distinction made between '36 months' and '36 Months.' To ensure consistency and improve data clarity, these two variations will be combined into a single variable labeled as '36 months.","metadata":{}},{"cell_type":"code","source":"# Convert the \"term\" values to lowercase\ndf['term'] = df['term'].str.lower()\n\n# Define a mapping dictionary to combine the values\nterm_mapping = {\n    \"36 months\": \"36 months\",\n    \"60 months\": \"60 months\"\n}\n\n# Update the \"term\" column with the new combined values using map\ndf['term'] = df['term'].replace(term_mapping)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:25.942333Z","iopub.execute_input":"2023-08-16T11:39:25.943218Z","iopub.status.idle":"2023-08-16T11:39:25.965568Z","shell.execute_reply.started":"2023-08-16T11:39:25.943189Z","shell.execute_reply":"2023-08-16T11:39:25.964420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['term'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:25.967027Z","iopub.execute_input":"2023-08-16T11:39:25.967941Z","iopub.status.idle":"2023-08-16T11:39:25.979715Z","shell.execute_reply.started":"2023-08-16T11:39:25.967909Z","shell.execute_reply":"2023-08-16T11:39:25.978554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Visualization","metadata":{}},{"cell_type":"code","source":"# Categorical columns.\ncategorical_features = df[[\"grade\", \"short_emp\", \"home_ownership\", \"purpose\", \"term\", \"last_delinq_none\", \"bad_loan\"]]\n\n# Numerical columns.\nnumerical_features = df[[\"annual_inc\", \"emp_length_num\", \"dti\", \"total_rec_late_fee\", \"revol_util\", \"od_ratio\"]]","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:25.981256Z","iopub.execute_input":"2023-08-16T11:39:25.981661Z","iopub.status.idle":"2023-08-16T11:39:25.995970Z","shell.execute_reply.started":"2023-08-16T11:39:25.981632Z","shell.execute_reply":"2023-08-16T11:39:25.995065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate descriptive statistics for categorical values.\ncategorical_features.astype('object').describe()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:25.997010Z","iopub.execute_input":"2023-08-16T11:39:25.998061Z","iopub.status.idle":"2023-08-16T11:39:26.059052Z","shell.execute_reply.started":"2023-08-16T11:39:25.998019Z","shell.execute_reply":"2023-08-16T11:39:26.058257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize the distribution of the \"bad_loan\" with \"grade\". \nsns.countplot(data=categorical_features, x=\"grade\", hue=\"bad_loan\")\nplt.xlabel(\"Grade\")\nplt.ylabel(\"Count\")\nplt.title(\"Count Plot of Grade with Bad Loan\")\nplt.legend(title=\"Bad Loan\", labels=[\"No\", \"Yes\"])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:26.060282Z","iopub.execute_input":"2023-08-16T11:39:26.060780Z","iopub.status.idle":"2023-08-16T11:39:26.395007Z","shell.execute_reply.started":"2023-08-16T11:39:26.060750Z","shell.execute_reply":"2023-08-16T11:39:26.394185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data illustrates a discernible trend where higher loan grades are consistently associated with lower instances of loan default. This trend is indicative of a strong negative correlation between loan grade and the likelihood of loan repayment failure.","metadata":{}},{"cell_type":"code","source":"# The percentage of each element of the data\nfor feature in categorical_features:\n    categorical_features[feature].value_counts().plot(kind = 'pie', autopct = '%1.1f%%')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:26.400374Z","iopub.execute_input":"2023-08-16T11:39:26.401225Z","iopub.status.idle":"2023-08-16T11:39:27.506157Z","shell.execute_reply.started":"2023-08-16T11:39:26.401185Z","shell.execute_reply":"2023-08-16T11:39:27.504674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evidently depicted in the visualization, there exists a noticeable imbalance within the variables \"short_emp,\" \"term,\" and \"bad_loan.\" As a countermeasure, a data resampling strategy will be implemented to rectify this imbalance. It's important to underscore that the distribution across the remaining dataset remains equitable and unaffected by this resampling process, thus preserving the integrity of the broader data structure.","metadata":{}},{"cell_type":"code","source":"# calculate descriptive statistics for numerical values.\nnumerical_features.describe()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:27.508085Z","iopub.execute_input":"2023-08-16T11:39:27.508874Z","iopub.status.idle":"2023-08-16T11:39:27.571707Z","shell.execute_reply.started":"2023-08-16T11:39:27.508819Z","shell.execute_reply":"2023-08-16T11:39:27.570308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Histograms for numerical features.\nnumerical_features.hist(bins=20, figsize=(12, 8))\nplt.suptitle(\"Histograms of Numerical Features\", y=1.02)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:27.573872Z","iopub.execute_input":"2023-08-16T11:39:27.574801Z","iopub.status.idle":"2023-08-16T11:39:28.863740Z","shell.execute_reply.started":"2023-08-16T11:39:27.574727Z","shell.execute_reply":"2023-08-16T11:39:28.862774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evidently, there are notable disparities in the distributions of \"total_rec_late_fee\" and \"revol_util,\" with \"annual_inc\" also exhibiting outliers. However, it's noteworthy that the distribution of the remaining data appears to be relatively balanced, devoid of significant irregularities.","metadata":{}},{"cell_type":"markdown","source":"# Data preprocessing","metadata":{}},{"cell_type":"markdown","source":"### Encoding and scalling the data","metadata":{}},{"cell_type":"code","source":"# One hot Endocing for \"home_ownership\" and \"purpose\".\ndf = pd.get_dummies(df, columns=['home_ownership', 'purpose', 'grade'])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:28.865360Z","iopub.execute_input":"2023-08-16T11:39:28.866050Z","iopub.status.idle":"2023-08-16T11:39:28.904672Z","shell.execute_reply.started":"2023-08-16T11:39:28.866010Z","shell.execute_reply":"2023-08-16T11:39:28.903973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a LabelEncoder object\nlabel_encoder = LabelEncoder()\n\n# Fit and transform the categorical column\ndf[\"term\"] = label_encoder.fit_transform(df[\"term\"])","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:28.905805Z","iopub.execute_input":"2023-08-16T11:39:28.906314Z","iopub.status.idle":"2023-08-16T11:39:28.917562Z","shell.execute_reply.started":"2023-08-16T11:39:28.906286Z","shell.execute_reply":"2023-08-16T11:39:28.916808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scalling the numerical data.\n\n# Create scaler object.\nscaler = StandardScaler()\n\n# Fit scaler on selected columns.\nscaler.fit(numerical_features)\n\n# Transform selected columns with scaler.\nnumerical_features = scaler.transform(numerical_features)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:28.918933Z","iopub.execute_input":"2023-08-16T11:39:28.919251Z","iopub.status.idle":"2023-08-16T11:39:28.933208Z","shell.execute_reply.started":"2023-08-16T11:39:28.919223Z","shell.execute_reply":"2023-08-16T11:39:28.931930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split the data","metadata":{}},{"cell_type":"code","source":"# Split data into x and y.\nX = df.drop(\"bad_loan\", axis=1)\ny = df[\"bad_loan\"]","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:28.935220Z","iopub.execute_input":"2023-08-16T11:39:28.935483Z","iopub.status.idle":"2023-08-16T11:39:28.941929Z","shell.execute_reply.started":"2023-08-16T11:39:28.935459Z","shell.execute_reply":"2023-08-16T11:39:28.941003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into train and test.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:28.944261Z","iopub.execute_input":"2023-08-16T11:39:28.944543Z","iopub.status.idle":"2023-08-16T11:39:28.964618Z","shell.execute_reply.started":"2023-08-16T11:39:28.944518Z","shell.execute_reply":"2023-08-16T11:39:28.963800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset will be split into two distinct groups to facilitate the implementation of over-sampling and under-sampling techniques. This strategy is devised to systematically assess and identify the optimal performance outcomes of both methodologies. By conducting controlled experiments with various machine learning models, the intention is to rigorously compare the effectiveness of both methods and subsequently determine the approach that yields the most favorable results.","metadata":{}},{"cell_type":"code","source":"# Split the train data into two subsets, train1 and test1\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X_train, y_train, test_size=0.25, random_state=10, stratify=y_train)\n\n# Split the train data into two subsets, train2 and test2\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X_train, y_train, test_size=0.25, random_state=20, stratify=y_train)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:28.966635Z","iopub.execute_input":"2023-08-16T11:39:28.967079Z","iopub.status.idle":"2023-08-16T11:39:28.992979Z","shell.execute_reply.started":"2023-08-16T11:39:28.967032Z","shell.execute_reply":"2023-08-16T11:39:28.992102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Resampling tha data","metadata":{}},{"cell_type":"markdown","source":"### Oversampling (SMOTE)","metadata":{}},{"cell_type":"code","source":"# Instantiate the SMOTE class\nsmote = SMOTE(sampling_strategy='auto', random_state=42)\n\n# Perform SMOTE oversampling on the dataset\nX_overesampled, y_overesampled = smote.fit_resample(X_train1, y_train1)\n\n# Visualize the distribution of values in the y_underesampled Series\ny_overesampled.value_counts().plot(kind = 'pie', autopct = '%1.1f%%')","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:28.994388Z","iopub.execute_input":"2023-08-16T11:39:28.994791Z","iopub.status.idle":"2023-08-16T11:39:29.162752Z","shell.execute_reply.started":"2023-08-16T11:39:28.994735Z","shell.execute_reply":"2023-08-16T11:39:29.160284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trying the models with oversampling","metadata":{}},{"cell_type":"markdown","source":"We will proceed to evaluate the models using oversampling techniques to determine which model exhibits superior performance. Given the imbalanced nature of the dataset, we will not rely solely on accuracy for model evaluation. Our initial focus will center on precision, as our priority lies in minimizing the instances of false positives—cases where the model predicts loan repayment, yet the borrower defaults.\n\nTo gain a comprehensive understanding of model performance, we will analyze the confusion matrix. This matrix will provide valuable insights into true positives, true negatives, false positives, and false negatives. This approach allows us to effectively navigate the intricacies of model performance in light of the dataset's inherent imbalance and make informed decisions about model selection.","metadata":{}},{"cell_type":"code","source":"# Initialize the models.\nmodels = {\n    'Random Forest': RandomForestClassifier(),\n    'Logistic Regression': LogisticRegression(),\n}","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:29.165040Z","iopub.execute_input":"2023-08-16T11:39:29.165947Z","iopub.status.idle":"2023-08-16T11:39:29.173534Z","shell.execute_reply.started":"2023-08-16T11:39:29.165896Z","shell.execute_reply":"2023-08-16T11:39:29.172107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Iterate over each model and evaluate its accuracy using cross-validation.\nfor model_name, model in models.items():\n    scores = cross_val_score(model, X_overesampled, y_overesampled)\n    accuracy = scores.mean()\n    print(f'{model_name} Accuracy: {accuracy}')\n    \n    # Fit the model to the full training set and make predictions on the test set\n    model.fit(X_overesampled, y_overesampled)\n    y_pred1 = model.predict(X_test1)\n    \n    # Evaluate the model on the test set\n    acc = accuracy_score(y_test1, y_pred1)\n    prec = precision_score(y_test1, y_pred1)\n    \n    print(f\"Accuracy: {acc:.3f}\")\n    print(f\"Precision: {prec:.3f}\")\n    print(confusion_matrix(y_test1, y_pred1))\n    print('-' * 50)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:29.175717Z","iopub.execute_input":"2023-08-16T11:39:29.176548Z","iopub.status.idle":"2023-08-16T11:39:43.368929Z","shell.execute_reply.started":"2023-08-16T11:39:29.176491Z","shell.execute_reply":"2023-08-16T11:39:43.367493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is clear that the logistic regression model exhibited better performance compared to the random forest model. This outcome signifies that the logistic regression algorithm was more adept at capturing the underlying patterns and nuances present in the dataset, leading to improved predictive accuracy.","metadata":{}},{"cell_type":"markdown","source":"### Undersampling (TomekLinks)","metadata":{}},{"cell_type":"code","source":"# Instantiate the TomekLinks class\ntomek_links = TomekLinks(sampling_strategy='auto', n_jobs=-1)\n\n# Perform Tomek Links undersampling on the dataset\nX_underesampled, y_underesampled = tomek_links.fit_resample(X_train2, y_train2)\n\n# Visualize the distribution of values in the y_underesampled Series\ny_underesampled.value_counts().plot(kind = 'pie', autopct = '%1.1f%%')","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:43.371465Z","iopub.execute_input":"2023-08-16T11:39:43.372589Z","iopub.status.idle":"2023-08-16T11:39:43.921541Z","shell.execute_reply.started":"2023-08-16T11:39:43.372530Z","shell.execute_reply":"2023-08-16T11:39:43.919833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trying the models with undersampling","metadata":{}},{"cell_type":"markdown","source":"We will also explore model performance using undersampling techniques to ensure a comprehensive evaluation of the models and select the most suitable approach. Employing undersampling involves reducing the number of instances from the majority class to balance the class distribution. This approach aims to mitigate the impact of class imbalance and allows us to observe how different models perform under varying data conditions.\n\nBy comparing the results obtained from both oversampling and undersampling approaches, we will make a well-informed decision regarding the optimal technique for our dataset. This comprehensive analysis will enable us to choose the model and technique that deliver the highest predictive accuracy and precision while accounting for the inherent complexities of imbalanced data.","metadata":{}},{"cell_type":"code","source":"# Iterate over each model and evaluate its accuracy using cross-validation.\nfor model_name, model in models.items():\n    scores = cross_val_score(model, X_underesampled, y_underesampled)\n    accuracy = scores.mean()\n    print(f'{model_name} Accuracy: {accuracy}')\n    \n    # Fit the model to the full training set and make predictions on the test set\n    model.fit(X_underesampled, y_underesampled)\n    y_pred2 = model.predict(X_test2)\n    \n    # Evaluate the model on the test set\n    acc = accuracy_score(y_test2, y_pred2)\n    prec = precision_score(y_test2, y_pred2)\n    \n    print(f\"Accuracy: {acc:.3f}\")\n    print(f\"Precision: {prec:.3f}\")\n    print(confusion_matrix(y_test2, y_pred2))\n    print(\"-\" * 50)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:43.928146Z","iopub.execute_input":"2023-08-16T11:39:43.928883Z","iopub.status.idle":"2023-08-16T11:39:53.171574Z","shell.execute_reply.started":"2023-08-16T11:39:43.928823Z","shell.execute_reply":"2023-08-16T11:39:53.170181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The performance analysis indicates that the random forest model outperforms the logistic regression model under the undersampling technique. Comparing the results obtained from both oversampling and undersampling approaches, we observe that the random forest model exhibits superior performance when paired with the undersampling technique.\n\nThis outcome underscores the effectiveness of the random forest algorithm in handling the complexities associated with imbalanced data, particularly when combined with undersampling to balance the class distribution. The results reaffirm the importance of tailoring the model and technique to the specific characteristics of the data at hand.","metadata":{}},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"markdown","source":"Finally, we will proceed to build the final model using the random forest algorithm combined with the undersampling technique. This choice is driven by the extensive performance analysis, which indicated that the random forest model, when used in conjunction with undersampling, yields the most favorable outcomes in terms of predictive accuracy and precision.\n\nBy leveraging the strengths of the random forest algorithm and addressing the class imbalance through undersampling, we aim to create a robust and effective model for assessing loan repayment probabilities. This final model will serve as a valuable tool for lending institutions, providing them with a reliable mechanism to make well-informed decisions, manage risks, and foster responsible lending practices.","metadata":{}},{"cell_type":"code","source":"# Perform SMOTE undrsampling on the dataset\nX_resampled, y_resampled = tomek_links.fit_resample(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:53.173734Z","iopub.execute_input":"2023-08-16T11:39:53.174633Z","iopub.status.idle":"2023-08-16T11:39:53.825440Z","shell.execute_reply.started":"2023-08-16T11:39:53.174584Z","shell.execute_reply":"2023-08-16T11:39:53.824560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the parameter grid.\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 5, 10],\n    'min_samples_split': [2, 5, 10]\n}\n\n# Create an instance of the model.\nmodel = RandomForestClassifier()\n\n# Create an instance of GridSearchCV.\ngrid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n\n# Fit the GridSearchCV to the data.\ngrid_search.fit(X_train, y_train)\n\n# Get the best hyperparameters.\nbest_params = grid_search.best_params_\n\n# Print the best hyperparameters.\nprint(\"Best hyperparameters: \", best_params)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:39:53.826704Z","iopub.execute_input":"2023-08-16T11:39:53.827204Z","iopub.status.idle":"2023-08-16T11:43:14.616889Z","shell.execute_reply.started":"2023-08-16T11:39:53.827174Z","shell.execute_reply":"2023-08-16T11:43:14.615789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make tha random forest model with specific best hyperparameters.\nmodel = RandomForestClassifier(max_depth= 10, min_samples_split= 10, n_estimators= 200)\n\n# Fit the model.\nmodel.fit(X_train, y_train)\n\n# Predict y-predict.\ny_pred = model.predict(X_test)\n\n# Evaluate the accuracy and precision of y-predict.\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\n    \nprint(f\"Accuracy: {acc:.3f}\")\nprint(f\"Precision: {prec:.3f}\")\nprint(confusion_matrix(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:43:14.620375Z","iopub.execute_input":"2023-08-16T11:43:14.620719Z","iopub.status.idle":"2023-08-16T11:43:17.932824Z","shell.execute_reply.started":"2023-08-16T11:43:14.620690Z","shell.execute_reply":"2023-08-16T11:43:17.931775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting predicted probabilities\ny_prob = model.predict_proba(X_test)[:, 1]\n\n# Calculate precision-recall curve\nprecision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n\n# Calculate average precision score\navg_precision = average_precision_score(y_test, y_prob)\n\n# Plot precision-recall curve\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, label=f'Precision-Recall Curve (Avg Precision = {avg_precision:.2f})')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend(loc='lower left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T11:43:17.934353Z","iopub.execute_input":"2023-08-16T11:43:17.934690Z","iopub.status.idle":"2023-08-16T11:43:18.401568Z","shell.execute_reply.started":"2023-08-16T11:43:17.934661Z","shell.execute_reply":"2023-08-16T11:43:18.400433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the final step, we reach the culmination of our process by creating the ultimate model. We choose the Random Forest algorithm after effectively using undersampling with TomekLinks. To enhance accuracy, we use Grid Search to fine-tune the model's settings and achieve the best possible performance.\n\nWe then turn our attention to a vital measure—how well the model predicts outcomes. Using a visual tool called the Precision-Recall Curve, we evaluate our model's effectiveness. The curve reveals an Average Precision score of 0.41. This value expresses the relative ability of the model to distinguish the positive category (the category that we aim to correctly predict) based on the level of accuracy and retrieval.","metadata":{}},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"In conclusion, we began by looking closely at the dataset and understanding the different information it contained. We noticed that there were no repeated entries, but some values were missing, which we removed. Then, we took a closer look at the data and made visual representations to better understand it.\n\nOne interesting thing we found was that loans with higher grades were less likely to have issues with repayment. We also realized that some information was not evenly balanced.\n\nBecause the data was not balanced, we tried different methods to fix this issue. We used techniques like making the groups larger or smaller (oversampling and undersampling). We applied two types of computer programs, Random Forest and Logistic Regression, to predict whether a loan might have problems. We compared how well these programs worked in terms of accuracy and precision. Our results showed that Random Forest worked best when we used a method called TomekLinks undersampling.\n\nThis journey involved carefully handling the data, choosing the right computer programs, and studying the results closely. The outcome is a solution that meets our goals of being very accurate and careful about risks. Ultimately, this effort helps create a financial system that's fair and stable.ConclusionConclusion","metadata":{}}]}